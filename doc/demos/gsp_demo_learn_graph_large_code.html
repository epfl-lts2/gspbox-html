 
<!DOCTYPE html>
<html lang="en">
<head class="include" file="../../include/header.html">
<link rel="stylesheet" href="../../include/bootstrap.min.css">
<link rel="stylesheet" href="../../include/bootstrap-theme.min.css">
<link rel="stylesheet" href="../../include/bootstrap-select.min.css">
<link rel="stylesheet" href="../../include/style.css" type="text/css">
<link rel="stylesheet" href="../../include/highlight.css" type="text/css">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta NAME="keywords" CONTENT="graph signal processing, graphs, filters, graph filters, signal processing, matlab,documentation"/>

<title>GSP_DEMO_LEARN_GRAPH_LARGE - Tutorial for graph learning using the GSPBox</title>
</head>



<!-- body must stay hidden until all include parts are loaded -->
<body style="display:none;">
<!-- Wrap the content into responsive container -->
<div class="container">
<!-- Include main navigation -->
    <div class="masthead include" file="../../include/mainnav.html"></div>
        <div class="row">
            <div class="col-md-2" id="codeswitch"><div id="menutitle"><a href="gsp_demo_learn_graph_large.html">View the help</a></div>
</div>
            <div class="col-md-offset-5 col-md-5" id="jumplist">This is where navigation should be.</div>
        </div>
        <div class="row">
            <div class="col-md-2">
                <div class="include" file='../include/docnav.html'></div>
                <br/>
                <div id="seealso"><p></p></div>
                <br/>
            </div>
            <div class="col-md-10">
           
                    <h1 class="title">GSP_DEMO_LEARN_GRAPH_LARGE - Tutorial for graph learning using the GSPBox</h1>
<h2>Program code:</h2>
<div class="highlight"><pre><span></span><span class="c">%GSP_DEMO_LEARN_GRAPH_LARGE Tutorial for graph learning using the GSPBox</span>
<span class="c">%</span>
<span class="c">%   This is a graph learning tutorial using GSP box. It deals with most</span>
<span class="c">%   aspects needed to know in order to use graph learning:</span>
<span class="c">%</span>
<span class="c">%   1) standard graph learning [Kalofolias 2016]</span>
<span class="c">%   2) setting params automatically for given sparsity [Kalofolias, Perraudin 2019]</span>
<span class="c">%   3) setting zero-edges up-front [Kalofolias, Perraudin 2019]</span>
<span class="c">%   4) large scale graph learning [Kalofolias, Perraudin 2019]</span>
<span class="c">%</span>
<span class="c">%   Let&#39;s create some artificial coordinates data:</span>
<span class="c">%</span>
<span class="c">%         gsp_reset_seed(1);</span>
<span class="c">%         G = gsp_2dgrid(16);</span>
<span class="c">%         n = G.N;</span>
<span class="c">%         W0 = G.W;</span>
<span class="c">%         G = gsp_graph(G.W, G.coords+.05*rand(size(G.coords)));</span>
<span class="c">%         figure; gsp_plot_graph(G);</span>
<span class="c">%</span>
<span class="c">%   Figure 1: Artificial data</span>
<span class="c">%</span>
<span class="c">% </span>
<span class="c">%</span>
<span class="c">%   Now suppose our data is the coordinates (x, y) of the points in space.</span>
<span class="c">%   Note that the data could often be a signal on these points, for example</span>
<span class="c">%   see GSP_DEMO_LEARN_GRAPH :</span>
<span class="c">%</span>
<span class="c">%         X = G.coords&#39;;</span>
<span class="c">%</span>
<span class="c">%   Right now the weights matrix W is all zeros and ones, but let&#39;s make it</span>
<span class="c">%   follow the geometry by learning weights from the new coordinates:</span>
<span class="c">% </span>
<span class="c">%         % First compute the matrix of all pairwise distances Z</span>
<span class="c">%         Z = gsp_distanz(X).^2;</span>
<span class="c">%         % this is a highly dense matrix</span>
<span class="c">%         figure; imagesc(Z);</span>
<span class="c">%         % Let&#39;s learn a graph, we need to know two parameters a and b according to</span>
<span class="c">%         % [1, Kalofolias, 2016]</span>
<span class="c">%         a = 1;</span>
<span class="c">%         b = 1;</span>
<span class="c">%         W = gsp_learn_graph_log_degrees(Z, a, b);</span>
<span class="c">%         % this matrix is naturally sparse, but still has many connections unless we</span>
<span class="c">%         % &quot;play&quot; with the parameters a and b to make it sparser</span>
<span class="c">%         figure; imagesc(W); </span>
<span class="c">%         % update weights</span>
<span class="c">%         W(W&lt;1e-4) = 0;</span>
<span class="c">%         G = gsp_update_weights(G, W);</span>
<span class="c">%</span>
<span class="c">%   Figure 2: Distance matrix</span>
<span class="c">%</span>
<span class="c">% </span>
<span class="c">%</span>
<span class="c">%   Figure 3: Learned weights using $a=1$ and $b=1$</span>
<span class="c">%</span>
<span class="c">% </span>
<span class="c">%</span>
<span class="c">%   A practical way of setting parameters is explained in our last</span>
<span class="c">%   publication [2, Kalofolias, Perraudin 2017]. Instead of setting a and</span>
<span class="c">%   b, we need one parameter theta changing sparsity. An automatic way to</span>
<span class="c">%   find a good approximation given a desired sparsity level is with</span>
<span class="c">%   function gsp_compute_graph_learning_theta :</span>
<span class="c">% </span>
<span class="c">%         % suppose we want 4 edges per node on average</span>
<span class="c">%         k = 4;</span>
<span class="c">%         theta = gsp_compute_graph_learning_theta(Z, k); </span>
<span class="c">%         % then instead of playing with a and b, we k keep them equal to 1 and</span>
<span class="c">%         % multiply Z by theta for graph learning:</span>
<span class="c">%         t1 = tic;</span>
<span class="c">%         [W, info_1] = gsp_learn_graph_log_degrees(theta  Z, 1, 1);</span>
<span class="c">%         t1 = toc(t1);</span>
<span class="c">%         % clean edges close to zero</span>
<span class="c">%         W(W&lt;1e-4) = 0; </span>
<span class="c">%         % indeed, the new adjacency matrix has around 4 non zeros per row!</span>
<span class="c">%         figure; imagesc(W); title([&#39;Average edges/node: &#39;, num2str(nnz(W)/G.N)]); </span>
<span class="c">%         % update weights and plot</span>
<span class="c">%         G = gsp_update_weights(G, W);</span>
<span class="c">%         figure; gsp_plot_graph(G);</span>
<span class="c">%</span>
<span class="c">%</span>
<span class="c">%   Figure 4: Learned weights using automatic parameter selection</span>
<span class="c">%</span>
<span class="c">% </span>
<span class="c">%</span>
<span class="c">%   Figure 5: Learned graph using automatic parameter selection</span>
<span class="c">%</span>
<span class="c">%</span>
<span class="c">%</span>
<span class="c">%   An interesting feature is that we can give a pattern of allowed edges,</span>
<span class="c">%   keeping all the others to zero. This is important to add contstraints</span>
<span class="c">%   to some problems, or just to reduce computations for others:</span>
<span class="c">% </span>
<span class="c">%         % Suppose for example that we know beforehand that no connection could or</span>
<span class="c">%         % should be formed before pairs of nodes with distance more than 0.02. We</span>
<span class="c">%         % create a mask with the pattern of allowed edges and pass it to the</span>
<span class="c">%         % learning algorithm in a parameters structure:</span>
<span class="c">%         params.edge_mask = zero_diag(Z &lt; 0.02);</span>
<span class="c">%         % we also set the flag fix_zeros to 1:</span>
<span class="c">%         params.fix_zeros = 1;</span>
<span class="c">%         [W2, info_2] = gsp_learn_graph_log_degrees(theta  Z, 1, 1, params);</span>
<span class="c">%         % clean edges close to zero</span>
<span class="c">%         W2(W2&lt;1e-4) = 0;</span>
<span class="c">%         % indeed, the new adjacency matrix has around 4 non zeros per row!</span>
<span class="c">%         figure; imagesc(W2); title([&#39;Average edges/node: &#39;, num2str(nnz(W2)/G.N)]);</span>
<span class="c">%         % update weights and plot</span>
<span class="c">%         G = gsp_update_weights(G, W2);</span>
<span class="c">%         figure; gsp_plot_graph(G);</span>
<span class="c">%         fprintf(&#39;The computation was %.1f times faster than before!n&#39;, info_1.time / info_2.time);</span>
<span class="c">%         fprintf(&#39;Relative difference between two solutions: %.4fn&#39;, norm(W-W2, &#39;fro&#39;)/norm(W, &#39;fro&#39;));</span>
<span class="c">%         % Note that the learned graph is sparser than the mask we gave as input.</span>
<span class="c">%         figure; subplot(1, 2, 1); spy(W2); title(&#39;W_2&#39;); subplot(1, 2, 2), spy(params.edge_mask); title(&#39;edge mask&#39;);</span>
<span class="c">%</span>
<span class="c">%   Figure 6: Learned weights using a pattern of allowed edges</span>
<span class="c">%</span>
<span class="c">% </span>
<span class="c">%</span>
<span class="c">%   Figure 7: Learned graph using a pattern of allowed edges</span>
<span class="c">%</span>
<span class="c">%</span>
<span class="c">%</span>
<span class="c">%   Figure 8: Edges selected by the algorithm</span>
<span class="c">%</span>
<span class="c">%</span>
<span class="c">%</span>
<span class="c">%   Actually this feature is the one that allows us to work with big data.</span>
<span class="c">%   Suppose the graph has n nodes. Classical graph learning [Kalofolias</span>
<span class="c">%   2016] costs O(n^2) computations. If the mask we give has O(n) edges,</span>
<span class="c">%   then the computation drops to O(n) as well. The question is, how can we</span>
<span class="c">%   compute efficiently a mask of O(n) allowed edges for general data, even</span>
<span class="c">%   if we don&#39;t have prior knowledge? Note that computing the full matrix Z</span>
<span class="c">%   already costs O(n^2) and we want to avoid it! The solution is</span>
<span class="c">%   Approximate Nearest Neighbors (ANN) that computes an approximate sparse</span>
<span class="c">%   matrix Z with much less computations, roughly O(nlog(n)). This is the</span>
<span class="c">%   idea behind [Kalofolias, Perraudin 2017] :</span>
<span class="c">%</span>
<span class="c">%         % We compute an approximate Nearest Neighbors graph (using the FLANN</span>
<span class="c">%         % library through GSP-box)</span>
<span class="c">%         params_NN.use_flann = 1;</span>
<span class="c">%         % we ask for an approximate k-NN graph with roughly double number of edges.</span>
<span class="c">%         % The +1 is because FLANN also counts each node as its own neighbor.</span>
<span class="c">%         params_NN.k = 2  k + 1;</span>
<span class="c">%         clock_flann = tic; </span>
<span class="c">%         [indx, indy, dist, ~, ~, ~, NN, Z_sorted] = gsp_nn_distanz(X, X, params_NN);</span>
<span class="c">%         time_flann = toc(clock_flann);</span>
<span class="c">%         fprintf(&#39;Time for FLANN: %.3f secondsn&#39;, toc(clock_flann));</span>
<span class="c">%         % gsp_nn_distanz gives distance matrix in a form ready to use with sparse:</span>
<span class="c">%         Z_sp = sparse(indx, indy, dist.^2, n, n, params_NN.k  n  2);</span>
<span class="c">%         % symmetrize the distance matrix</span>
<span class="c">%         Z_sp = gsp_symmetrize(Z_sp, &#39;full&#39;);</span>
<span class="c">%         % get rid or first row that is zero (each node has 0 distance from itself)</span>
<span class="c">%         Z_sorted = Z_sorted(:, 2:end).^2;   % first row is zero</span>
<span class="c">%         % Note that FLANN returns Z already sorted, that further saves computation</span>
<span class="c">%         % for computing the parameter theta.</span>
<span class="c">%         Z_is_sorted = true;</span>
<span class="c">%         theta = gsp_compute_graph_learning_theta(Z_sorted, k, 0, Z_is_sorted);</span>
<span class="c">%         params.edge_mask = Z_sp &gt; 0;</span>
<span class="c">%         params.fix_zeros = 1;</span>
<span class="c">%         [W3, info_3] = gsp_learn_graph_log_degrees(Z_sp  theta, 1, 1, params);</span>
<span class="c">%         W3(W3&lt;1e-4) = 0;</span>
<span class="c">%         fprintf(&#39;Relative difference between two solutions: %.4fn&#39;, norm(W-W2, &#39;fro&#39;)/norm(W, &#39;fro&#39;));</span>
<span class="c">%         % Note that the learned graph is sparser than the mask we gave as input.</span>
<span class="c">%         figure; subplot(1, 2, 1); spy(W3); title(&#39;W_3&#39;); subplot(1, 2, 2), spy(params.edge_mask); title(&#39;edge mask&#39;);</span>
<span class="c">%</span>
<span class="c">%   Figure 9: Edges selected by the algorithm</span>
<span class="c">%</span>
<span class="c">%</span>
<span class="c">%</span>
<span class="c">%   Note that we can alternatively use the vector form of Z, W, and the</span>
<span class="c">%   mask, but always AFTER symmetrizing the distance matrix Z. Make sure</span>
<span class="c">%   you use our sparse version squareform_sp.m instead of Matlab&#39;s native</span>
<span class="c">%   squareform.m, or for big graphs you might have memory issues:</span>
<span class="c">% </span>
<span class="c">%         z_sp = squareform_sp(Z_sp);</span>
<span class="c">%         params.edge_mask = z_sp &gt; 0;</span>
<span class="c">%         % the output will be also in vectorform if the distances were in vectorform</span>
<span class="c">%         [w3, info_3_sp] = gsp_learn_graph_log_degrees(z_sp  theta, 1, 1, params);</span>
<span class="c">%         w3(w3&lt;1e-4) = 0;</span>
<span class="c">%         norm(w3 - squareform_sp(W3)) / norm(w3)</span>
<span class="c">%</span>
<span class="c">%   Enjoy!</span>
<span class="c">%</span>
<span class="c">%   See also: gsp_demo_learn_graph</span>
<span class="c">%</span>
<span class="c">%   References:</span>
<span class="c">%     V. Kalofolias. How to learn a graph from smooth signals. Technical</span>
<span class="c">%     report, AISTATS 2016: proceedings at Journal of Machine Learning</span>
<span class="c">%     Research (JMLR)., 2016.</span>
<span class="c">%     </span>
<span class="c">%     V. Kalofolias and N. Perraudin. Large scale graph learning from smooth</span>
<span class="c">%     signals. International Conference on Learning Representations, 2019.</span>
<span class="c">%     </span>
<span class="c">%</span>
<span class="c">%   Url: https://epfl-lts2.github.io/gspbox-html/doc/demos/gsp_demo_learn_graph_large.html</span>

<span class="c">% Copyright (C) 2013-2016 Nathanael Perraudin, Johan Paratte, David I Shuman.</span>
<span class="c">% This file is part of GSPbox version 0.7.5</span>
<span class="c">%</span>
<span class="c">% This program is free software: you can redistribute it and/or modify</span>
<span class="c">% it under the terms of the GNU General Public License as published by</span>
<span class="c">% the Free Software Foundation, either version 3 of the License, or</span>
<span class="c">% (at your option) any later version.</span>
<span class="c">%</span>
<span class="c">% This program is distributed in the hope that it will be useful,</span>
<span class="c">% but WITHOUT ANY WARRANTY; without even the implied warranty of</span>
<span class="c">% MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</span>
<span class="c">% GNU General Public License for more details.</span>
<span class="c">%</span>
<span class="c">% You should have received a copy of the GNU General Public License</span>
<span class="c">% along with this program.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</span>

<span class="c">% If you use this toolbox please kindly cite</span>
<span class="c">%     N. Perraudin, J. Paratte, D. Shuman, V. Kalofolias, P. Vandergheynst,</span>
<span class="c">%     and D. K. Hammond. GSPBOX: A toolbox for signal processing on graphs.</span>
<span class="c">%     ArXiv e-prints, Aug. 2014.</span>
<span class="c">% http://arxiv.org/abs/1408.5781</span>

<span class="c">% code author: Vassilis Kalofolias</span>
<span class="c">% date: summer 2017</span>
<span class="c">% test_prox_log_sum (for automatization of regularization constants)</span>

<span class="n">warning</span> <span class="n">off</span>

<span class="c">%% Let&#39;s create some artificial coordinates data</span>
<span class="n">gsp_reset_seed</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="n">G</span> <span class="p">=</span> <span class="n">gsp_2dgrid</span><span class="p">(</span><span class="mi">16</span><span class="p">);</span>
<span class="n">n</span> <span class="p">=</span> <span class="n">G</span><span class="p">.</span><span class="n">N</span><span class="p">;</span>
<span class="n">W0</span> <span class="p">=</span> <span class="n">G</span><span class="p">.</span><span class="n">W</span><span class="p">;</span>
<span class="n">G</span> <span class="p">=</span> <span class="n">gsp_graph</span><span class="p">(</span><span class="n">G</span><span class="p">.</span><span class="n">W</span><span class="p">,</span> <span class="n">G</span><span class="p">.</span><span class="n">coords</span><span class="o">+</span><span class="p">.</span><span class="mi">05</span><span class="o">*</span><span class="nb">rand</span><span class="p">(</span><span class="nb">size</span><span class="p">(</span><span class="n">G</span><span class="p">.</span><span class="n">coords</span><span class="p">)));</span>
<span class="n">figure</span><span class="p">;</span> <span class="n">gsp_plot_graph</span><span class="p">(</span><span class="n">G</span><span class="p">);</span>

<span class="c">% Now suppose our data is the coordinates (x, y) of the points in space.</span>
<span class="c">% Note that the data could often be a signal on these points, for example</span>
<span class="c">% see gsp_demo_learn_graph.</span>
<span class="n">X</span> <span class="p">=</span> <span class="n">G</span><span class="p">.</span><span class="n">coords</span><span class="o">&#39;</span><span class="p">;</span>

<span class="c">%% Right now the weights matrix W is all zeros and ones, but let&#39;s make it </span>
<span class="c">%% follow the geometry by learning weights from the new coordinates:</span>

<span class="c">% First compute the matrix of all pairwise distances Z</span>
<span class="n">Z</span> <span class="p">=</span> <span class="n">gsp_distanz</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.^</span><span class="mi">2</span><span class="p">;</span>
<span class="c">% this is a highly dense matrix</span>
<span class="n">figure</span><span class="p">;</span> <span class="n">imagesc</span><span class="p">(</span><span class="n">Z</span><span class="p">);</span>

<span class="c">% Let&#39;s learn a graph, we need to know two parameters a and b according to</span>
<span class="c">% [Kalofolias, 2016]</span>
<span class="n">a</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="n">b</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="n">W</span> <span class="p">=</span> <span class="n">gsp_learn_graph_log_degrees</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">);</span>

<span class="c">% this matrix is naturally sparse, but still has many connections unless we</span>
<span class="c">% &quot;play&quot; with the parameters a and b to make it sparser</span>
<span class="n">figure</span><span class="p">;</span> <span class="n">imagesc</span><span class="p">(</span><span class="n">W</span><span class="p">);</span>

<span class="c">% update weights and plot</span>
<span class="n">W</span><span class="p">(</span><span class="n">W</span><span class="o">&lt;</span><span class="mf">1e-4</span><span class="p">)</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">G</span> <span class="p">=</span> <span class="n">gsp_update_weights</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">W</span><span class="p">);</span>
<span class="c">% figure; gsp_plot_graph(G);</span>

<span class="c">%% A practical way of setting parameters is explained in our last </span>
<span class="c">%% publication [Kalofolias, Perraudin 2017].</span>
<span class="c">% Instead of setting a and b, we need one parameter theta changing</span>
<span class="c">% sparsity. An automatic way to find a good approximation given a desired</span>
<span class="c">% sparsity level is with function gsp_compute_graph_learning_theta. </span>

<span class="c">% suppose we want 4 edges per node on average</span>
<span class="n">k</span> <span class="p">=</span> <span class="mi">4</span><span class="p">;</span>
<span class="n">theta</span> <span class="p">=</span> <span class="n">gsp_compute_graph_learning_theta</span><span class="p">(</span><span class="n">Z</span><span class="p">,</span> <span class="n">k</span><span class="p">);</span>

<span class="c">% then instead of playing with a and b, we k keep them equal to 1 and</span>
<span class="c">% multiply Z by theta for graph learning:</span>
<span class="n">t1</span> <span class="p">=</span> <span class="n">tic</span><span class="p">;</span>
<span class="p">[</span><span class="n">W</span><span class="p">,</span> <span class="n">info_1</span><span class="p">]</span> <span class="p">=</span> <span class="n">gsp_learn_graph_log_degrees</span><span class="p">(</span><span class="n">theta</span> <span class="o">*</span> <span class="n">Z</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span>
<span class="n">t1</span> <span class="p">=</span> <span class="n">toc</span><span class="p">(</span><span class="n">t1</span><span class="p">);</span>
<span class="c">% clean edges close to zero</span>
<span class="n">W</span><span class="p">(</span><span class="n">W</span><span class="o">&lt;</span><span class="mf">1e-4</span><span class="p">)</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span>

<span class="c">% indeed, the new adjacency matrix has around 4 non zeros per row!</span>
<span class="n">figure</span><span class="p">;</span> <span class="n">imagesc</span><span class="p">(</span><span class="n">W</span><span class="p">);</span> <span class="n">title</span><span class="p">([</span><span class="s">&#39;Average edges/node: &#39;</span><span class="p">,</span> <span class="n">num2str</span><span class="p">(</span><span class="n">nnz</span><span class="p">(</span><span class="n">W</span><span class="p">)</span><span class="o">/</span><span class="n">G</span><span class="p">.</span><span class="n">N</span><span class="p">)]);</span>

<span class="c">% update weights and plot</span>
<span class="n">G</span> <span class="p">=</span> <span class="n">gsp_update_weights</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">W</span><span class="p">);</span>
<span class="n">figure</span><span class="p">;</span> <span class="n">gsp_plot_graph</span><span class="p">(</span><span class="n">G</span><span class="p">);</span>



<span class="c">%% An interesting feature is that we can give a pattern of allowed edges, </span>
<span class="c">%% keeping all the others to zero. This is important to add contstraints to</span>
<span class="c">%% some problems, or just to reduce computations for others.</span>

<span class="c">% Suppose for example that we know beforehand that no connection could or</span>
<span class="c">% should be formed before pairs of nodes with distance more than 0.02. We</span>
<span class="c">% create a mask with the pattern of allowed edges and pass it to the</span>
<span class="c">% learning algorithm in a parameters structure:</span>
<span class="n">params</span><span class="p">.</span><span class="n">edge_mask</span> <span class="p">=</span> <span class="n">zero_diag</span><span class="p">(</span><span class="n">Z</span> <span class="o">&lt;</span> <span class="mf">0.02</span><span class="p">);</span>
<span class="c">% we also set the flag fix_zeros to 1:</span>
<span class="n">params</span><span class="p">.</span><span class="n">fix_zeros</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>

<span class="p">[</span><span class="n">W2</span><span class="p">,</span> <span class="n">info_2</span><span class="p">]</span> <span class="p">=</span> <span class="n">gsp_learn_graph_log_degrees</span><span class="p">(</span><span class="n">theta</span> <span class="o">*</span> <span class="n">Z</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">params</span><span class="p">);</span>

<span class="c">% clean edges close to zero</span>
<span class="n">W2</span><span class="p">(</span><span class="n">W2</span><span class="o">&lt;</span><span class="mf">1e-4</span><span class="p">)</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span>

<span class="c">% indeed, the new adjacency matrix has around 4 non zeros per row!</span>
<span class="n">figure</span><span class="p">;</span> <span class="n">imagesc</span><span class="p">(</span><span class="n">W2</span><span class="p">);</span> <span class="n">title</span><span class="p">([</span><span class="s">&#39;Average edges/node: &#39;</span><span class="p">,</span> <span class="n">num2str</span><span class="p">(</span><span class="n">nnz</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span><span class="o">/</span><span class="n">G</span><span class="p">.</span><span class="n">N</span><span class="p">)]);</span>

<span class="c">% update weights and plot</span>
<span class="n">G</span> <span class="p">=</span> <span class="n">gsp_update_weights</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">W2</span><span class="p">);</span>
<span class="n">figure</span><span class="p">;</span> <span class="n">gsp_plot_graph</span><span class="p">(</span><span class="n">G</span><span class="p">);</span>

<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39;The computation was %.1f times faster than before!\n&#39;</span><span class="p">,</span> <span class="n">info_1</span><span class="p">.</span><span class="n">time</span> <span class="o">/</span> <span class="n">info_2</span><span class="p">.</span><span class="n">time</span><span class="p">);</span>
<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39;Relative difference between two solutions: %.4f\n&#39;</span><span class="p">,</span> <span class="n">norm</span><span class="p">(</span><span class="n">W</span><span class="o">-</span><span class="n">W2</span><span class="p">,</span> <span class="s">&#39;fro&#39;</span><span class="p">)</span><span class="o">/</span><span class="n">norm</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="s">&#39;fro&#39;</span><span class="p">));</span>
<span class="c">% Note that the learned graph is sparser than the mask we gave as input.</span>

<span class="n">figure</span><span class="p">;</span> <span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="n">spy</span><span class="p">(</span><span class="n">W2</span><span class="p">);</span> <span class="n">title</span><span class="p">(</span><span class="s">&#39;W_2&#39;</span><span class="p">);</span> <span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">spy</span><span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">edge_mask</span><span class="p">);</span> <span class="n">title</span><span class="p">(</span><span class="s">&#39;edge mask&#39;</span><span class="p">);</span>


<span class="c">%% Actually this feature is the one that allows us to work with big data. </span>
<span class="c">%% Suppose the graph has n nodes. Classical graph learning [Kalofolias 2016]</span>
<span class="c">%% costs O(n^2) computations. If the mask we give has O(n) edges, then the</span>
<span class="c">%% computation drops to O(n) as well. The question is, how can we compute</span>
<span class="c">%% efficiently a mask of O(n) allowed edges for general data, even if we </span>
<span class="c">%% don&#39;t have prior knowledge? Note that computing the full matrix Z </span>
<span class="c">%% already costs O(n^2) and we want to avoid it! The solution is Approximate</span>
<span class="c">%% Nearest Neighbors (ANN) that computes an approximate sparse matrix Z with</span>
<span class="c">%% much less computations, roughly O(nlog(n)). This is the idea behind</span>
<span class="c">%% [Kalofolias, Perraudin 2017]</span>

<span class="c">% We compute an approximate Nearest Neighbors graph (using the FLANN</span>
<span class="c">% library through GSP-box)</span>
<span class="n">params_NN</span><span class="p">.</span><span class="n">use_flann</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>
<span class="c">% we ask for an approximate k-NN graph with roughly double number of edges.</span>
<span class="c">% The +1 is because FLANN also counts each node as its own neighbor.</span>
<span class="n">params_NN</span><span class="p">.</span><span class="n">k</span> <span class="p">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">k</span> <span class="o">+</span> <span class="mi">1</span><span class="p">;</span>


<span class="n">clock_flann</span> <span class="p">=</span> <span class="n">tic</span><span class="p">;</span> 
<span class="p">[</span><span class="n">indx</span><span class="p">,</span> <span class="n">indy</span><span class="p">,</span> <span class="n">dist</span><span class="p">,</span> <span class="o">~</span><span class="p">,</span> <span class="o">~</span><span class="p">,</span> <span class="o">~</span><span class="p">,</span> <span class="n">NN</span><span class="p">,</span> <span class="n">Z_sorted</span><span class="p">]</span> <span class="p">=</span> <span class="n">gsp_nn_distanz</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">params_NN</span><span class="p">);</span>
<span class="n">time_flann</span> <span class="p">=</span> <span class="n">toc</span><span class="p">(</span><span class="n">clock_flann</span><span class="p">);</span>
<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39;Time for FLANN: %.3f seconds\n&#39;</span><span class="p">,</span> <span class="n">toc</span><span class="p">(</span><span class="n">clock_flann</span><span class="p">));</span>

<span class="c">% gsp_nn_distanz gives distance matrix in a form ready to use with sparse:</span>
<span class="n">Z_sp</span> <span class="p">=</span> <span class="n">sparse</span><span class="p">(</span><span class="n">indx</span><span class="p">,</span> <span class="n">indy</span><span class="p">,</span> <span class="n">dist</span><span class="o">.^</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">n</span><span class="p">,</span> <span class="n">params_NN</span><span class="p">.</span><span class="n">k</span> <span class="o">*</span> <span class="n">n</span> <span class="o">*</span> <span class="mi">2</span><span class="p">);</span>
<span class="c">% symmetrize the distance matrix</span>
<span class="n">Z_sp</span> <span class="p">=</span> <span class="n">gsp_symmetrize</span><span class="p">(</span><span class="n">Z_sp</span><span class="p">,</span> <span class="s">&#39;full&#39;</span><span class="p">);</span>
<span class="c">% get rid or first row that is zero (each node has 0 distance from itself)</span>
<span class="n">Z_sorted</span> <span class="p">=</span> <span class="n">Z_sorted</span><span class="p">(:,</span> <span class="mi">2</span><span class="p">:</span><span class="k">end</span><span class="p">)</span><span class="o">.^</span><span class="mi">2</span><span class="p">;</span>   <span class="c">% first row is zero</span>

<span class="c">% Note that FLANN returns Z already sorted, that further saves computation</span>
<span class="c">% for computing the parameter theta.</span>
<span class="n">Z_is_sorted</span> <span class="p">=</span> <span class="n">true</span><span class="p">;</span>
<span class="n">theta</span> <span class="p">=</span> <span class="n">gsp_compute_graph_learning_theta</span><span class="p">(</span><span class="n">Z_sorted</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">Z_is_sorted</span><span class="p">);</span>

<span class="n">params</span><span class="p">.</span><span class="n">edge_mask</span> <span class="p">=</span> <span class="n">Z_sp</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span>
<span class="n">params</span><span class="p">.</span><span class="n">fix_zeros</span> <span class="p">=</span> <span class="mi">1</span><span class="p">;</span>

<span class="p">[</span><span class="n">W3</span><span class="p">,</span> <span class="n">info_3</span><span class="p">]</span> <span class="p">=</span> <span class="n">gsp_learn_graph_log_degrees</span><span class="p">(</span><span class="n">Z_sp</span> <span class="o">*</span> <span class="n">theta</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">params</span><span class="p">);</span>
<span class="n">W3</span><span class="p">(</span><span class="n">W3</span><span class="o">&lt;</span><span class="mf">1e-4</span><span class="p">)</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span>

<span class="n">fprintf</span><span class="p">(</span><span class="s">&#39;Relative difference between two solutions: %.4f\n&#39;</span><span class="p">,</span> <span class="n">norm</span><span class="p">(</span><span class="n">W</span><span class="o">-</span><span class="n">W2</span><span class="p">,</span> <span class="s">&#39;fro&#39;</span><span class="p">)</span><span class="o">/</span><span class="n">norm</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="s">&#39;fro&#39;</span><span class="p">));</span>
<span class="c">% Note that the learned graph is sparser than the mask we gave as input.</span>

<span class="n">figure</span><span class="p">;</span> <span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">);</span> <span class="n">spy</span><span class="p">(</span><span class="n">W3</span><span class="p">);</span> <span class="n">title</span><span class="p">(</span><span class="s">&#39;W_3&#39;</span><span class="p">);</span> <span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">spy</span><span class="p">(</span><span class="n">params</span><span class="p">.</span><span class="n">edge_mask</span><span class="p">);</span> <span class="n">title</span><span class="p">(</span><span class="s">&#39;edge mask&#39;</span><span class="p">);</span>


<span class="c">%% Note that we can alternatively use the vector form of Z, W, and the mask,</span>
<span class="c">%% but always AFTER symmetrizing the distance matrix Z. Make sure you use </span>
<span class="c">%% our sparse version squareform_sp.m instead of Matlab&#39;s native squareform.m,</span>
<span class="c">%% or for big graphs you might have memory issues</span>

<span class="n">z_sp</span> <span class="p">=</span> <span class="n">squareform_sp</span><span class="p">(</span><span class="n">Z_sp</span><span class="p">);</span>
<span class="n">params</span><span class="p">.</span><span class="n">edge_mask</span> <span class="p">=</span> <span class="n">z_sp</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">;</span>
<span class="c">% the output will be also in vectorform if the distances were in vectorform</span>
<span class="p">[</span><span class="n">w3</span><span class="p">,</span> <span class="n">info_3_sp</span><span class="p">]</span> <span class="p">=</span> <span class="n">gsp_learn_graph_log_degrees</span><span class="p">(</span><span class="n">z_sp</span> <span class="o">*</span> <span class="n">theta</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">params</span><span class="p">);</span>
<span class="n">w3</span><span class="p">(</span><span class="n">w3</span><span class="o">&lt;</span><span class="mf">1e-4</span><span class="p">)</span> <span class="p">=</span> <span class="mi">0</span><span class="p">;</span>

<span class="n">norm</span><span class="p">(</span><span class="n">w3</span> <span class="o">-</span> <span class="n">squareform_sp</span><span class="p">(</span><span class="n">W3</span><span class="p">))</span> <span class="o">/</span> <span class="n">norm</span><span class="p">(</span><span class="n">w3</span><span class="p">)</span>
</pre></div>


            </div>
        </div>

        <div class="include" file="../../include/footer.html"></div>
    </div>
</div>
<!-- These two have to be here to dynamically load the included parts -->
<script src="../../include/jquery.min.js"></script>
<script src="../../include/bootstrap-select.min.js"></script>
<script src="../../include/gspbox.js" type="text/javascript"></script>
<script src="../include/lookup.js" type="text/javascript"></script>
<script src="../include/jumplist.js" type="text/javascript"></script>
</body>
</html>




